# MLOps California Housing Prediction Platform

A complete MLOps pipeline for California Housing price prediction with DVC data versioning, comprehensive data management, and robust validation systems. This project demonstrates production-ready MLOps practices with automated data processing, quality validation, and feature engineering.

## ğŸ—ï¸ Architecture Overview

```
MLOps Platform
â”œâ”€â”€ Data Management (DVC)
â”‚   â”œâ”€â”€ California Housing Dataset
â”‚   â”œâ”€â”€ Data Validation & Quality Checks
â”‚   â””â”€â”€ Version Control with Remote Storage
â”œâ”€â”€ Model Development (MLflow)
â”‚   â”œâ”€â”€ Multiple ML Models (XGBoost, Neural Networks, etc.)
â”‚   â”œâ”€â”€ Experiment Tracking
â”‚   â””â”€â”€ Model Registry
â”œâ”€â”€ API Deployment (FastAPI)
â”‚   â”œâ”€â”€ Prediction Endpoints
â”‚   â”œâ”€â”€ Model Serving
â”‚   â””â”€â”€ Input Validation
â”œâ”€â”€ CI/CD Pipeline (GitHub Actions)
â”‚   â”œâ”€â”€ Automated Testing
â”‚   â”œâ”€â”€ Docker Build & Deploy
â”‚   â””â”€â”€ Quality Gates
â””â”€â”€ Monitoring & Logging
    â”œâ”€â”€ Prediction Logging
    â”œâ”€â”€ Model Performance Tracking
    â””â”€â”€ System Health Monitoring
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Git
- 4GB+ RAM (for ML model training)

### 1. Clone Repository
```bash
git clone https://github.com/Somdatta-dev/mlops-california-housing.git
cd mlops-california-housing
```

### 2. Automated Setup (Recommended)
```bash
python setup_project.py
```

This script will:
- âœ… Install all dependencies
- âœ… Set up DVC remote storage
- âœ… Pull dataset from DVC
- âœ… Verify data integrity
- âœ… Test data loading

### 3. Manual Setup (Alternative)
```bash
# Install dependencies
pip install -r requirements.txt

# Create DVC remote storage
mkdir ../dvc_remote_storage

# Pull data from DVC
dvc pull

# Verify setup
python src/data_validation.py
```

## ğŸ“Š Dataset Information

**California Housing Dataset**
- **Samples**: 20,640 housing records
- **Features**: 8 numerical features
- **Target**: Median house value
- **Source**: sklearn.datasets.fetch_california_housing

**Features:**
- `MedInc`: Median income in block group
- `HouseAge`: Median house age in block group  
- `AveRooms`: Average number of rooms per household
- `AveBedrms`: Average number of bedrooms per household
- `Population`: Block group population
- `AveOccup`: Average number of household members
- `Latitude`: Block group latitude
- `Longitude`: Block group longitude

## ğŸ”§ Project Structure

```
mlops-california-housing/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw dataset files (DVC tracked)
â”‚   â”œâ”€â”€ processed/              # Processed data splits (DVC tracked)
â”‚   â””â”€â”€ interim/                # Intermediate processing files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_manager.py         # Core data management with DVC integration
â”‚   â”œâ”€â”€ data_loader.py          # Data loading utilities
â”‚   â”œâ”€â”€ data_validation.py      # Data quality validation
â”‚   â””â”€â”€ setup_dvc_remote.py     # DVC remote configuration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_manager.py    # Comprehensive data management tests
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for EDA
â”œâ”€â”€ docker/                     # Docker configuration
â”œâ”€â”€ .github/workflows/          # CI/CD pipelines
â”œâ”€â”€ .kiro/specs/mlops-platform/ # Project specifications
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup_project.py           # Automated setup script
â”œâ”€â”€ .env                       # Environment configuration
â””â”€â”€ README.md                  # This file
```

## ğŸ› ï¸ Development Workflow

### Data Management
```bash
# Run complete data management pipeline
python src/data_manager.py

# Individual components
python src/data_loader.py          # Load California Housing dataset
python src/data_validation.py      # Validate data quality
python src/setup_dvc_remote.py     # Configure DVC remote storage

# Check DVC status
dvc status
dvc pull  # Pull latest data
dvc push  # Push data changes
```

### Testing
```bash
# Run comprehensive test suite
pytest tests/test_data_manager.py -v

# Run all tests
pytest tests/ -v

# Run with coverage
pytest --cov=src tests/
```

### Data Processing Features
```bash
# The DataManager provides:
# - Automatic data download and validation
# - Feature engineering (8 â†’ 16 features)
# - Outlier handling with IQR/Z-score methods
# - Train/validation/test splits (64%/16%/20%)
# - Data scaling with StandardScaler/RobustScaler
# - Comprehensive quality reporting
```

## ğŸ“ˆ Data Management & Validation

### Core DataManager Features âœ…

**Comprehensive Data Pipeline:**
- **Pydantic Data Models**: Strict validation with CaliforniaHousingData model
- **Feature Engineering**: Automatically creates 8 additional features from original 8
- **Quality Validation**: Schema validation, missing values, duplicates, outliers detection
- **Outlier Handling**: IQR and Z-score methods with configurable thresholds
- **Data Scaling**: StandardScaler and RobustScaler options
- **Train/Val/Test Splits**: Configurable ratios (default: 64%/16%/20%)

**Engineered Features:**
- `RoomsPerHousehold`: Average rooms per household
- `BedroomsPerRoom`: Bedroom to room ratio
- `PopulationPerHousehold`: Population density per household
- `DistanceFromCenter`: Geographic distance from dataset center
- `IncomePerRoom`: Income normalized by rooms
- `IncomePerPerson`: Income per occupant
- `HouseAgeCategory`: Categorical age groups (0-4 scale)
- `PopulationDensity`: Population per room per occupant

### Data Validation Results

The dataset passes comprehensive validation with the following checks:
- âœ… **Schema Validation**: All expected columns present with correct types
- âœ… **Data Quality**: No missing values, no duplicates, no infinite values
- âœ… **Statistical Properties**: Reasonable distributions and correlations
- âœ… **Pydantic Validation**: All records conform to CaliforniaHousingData model
- âš ï¸ **Value Ranges**: Some outliers detected and handled (normal for real-world data)

**Dataset Statistics:**
- Total samples: 20,640
- Original features: 8 numerical
- Engineered features: 8 additional
- Missing values: 0
- Duplicate rows: 0
- Data quality score: 100%

## ğŸ”„ DVC Data Versioning

This project uses DVC (Data Version Control) for dataset management:

**Current Configuration:**
- **Remote Storage**: Local directory (`../dvc_remote_storage`)
- **Tracked Files**: 
  - `california_housing_features.csv` (1.8MB)
  - `california_housing_targets.csv` (164KB)
  - `dataset_metadata.json` (2KB)
  - `validation_report.json` (4KB)

**DVC Commands:**
```bash
dvc status    # Check data status
dvc pull      # Download data from remote
dvc push      # Upload data to remote
dvc add       # Track new data files
```

## ğŸ³ Docker Support

```bash
# Build Docker image (when implemented)
docker build -t mlops-housing .

# Run container
docker run -p 8000:8000 mlops-housing
```

## ğŸ” Monitoring & Logging

The platform includes comprehensive monitoring:
- **Data Quality**: Automated validation reports
- **Model Performance**: MLflow experiment tracking
- **API Metrics**: Request/response logging
- **System Health**: Resource usage monitoring

## ğŸ§ª Testing Strategy

**Comprehensive Test Suite (23 Tests):**
- **Pydantic Model Tests**: CaliforniaHousingData validation
- **DataManager Tests**: Core functionality, DVC integration, preprocessing
- **Data Quality Tests**: Validation, outlier handling, feature engineering
- **Integration Tests**: Full pipeline testing with real data

```bash
# Run all data management tests
pytest tests/test_data_manager.py -v

# Run specific test classes
pytest tests/test_data_manager.py::TestCaliforniaHousingData -v
pytest tests/test_data_manager.py::TestDataManager -v
pytest tests/test_data_manager.py::TestIntegration -v

# Run with coverage
pytest --cov=src tests/
```

## ğŸ“š API Documentation

Once the API is implemented, documentation will be available at:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸš€ Deployment

### Local Development
```bash
python setup_project.py
uvicorn src.api.main:app --reload
```

### Production (Docker)
```bash
docker-compose up -d
```

### Cloud Deployment
- AWS EC2 with Docker
- Google Cloud Run
- Azure Container Instances

## ğŸ”§ Configuration

Environment variables (`.env`):
```bash
# MLflow Configuration
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=california-housing-prediction

# API Configuration  
API_HOST=0.0.0.0
API_PORT=8000

# Database Configuration
DATABASE_URL=sqlite:///./mlops_platform.db
```



## ğŸ†˜ Troubleshooting

### Common Issues

**DVC Pull Fails:**
```bash
# Check remote configuration
dvc remote list

# Recreate remote storage
mkdir ../dvc_remote_storage
dvc push
```

**Data Loading Errors:**
```bash
# Verify data files exist
ls -la data/raw/

# Re-download dataset
python src/data_loader.py
```

**Import Errors:**
```bash
# Reinstall dependencies
pip install -r requirements.txt

# Check Python path
python -c "import sys; print(sys.path)"
```


Built with â¤ï¸ for MLOps best practices